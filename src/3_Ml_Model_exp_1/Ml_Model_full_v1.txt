anower@anower-P453UJ:~/All/Python/Thesis_VS_code$ /usr/bin/python3 /home/anower/All/Python/Thesis_VS_code/full_epoch/3_Ml_Model/ML_Model_full_v2.py
======================================================================
EXPERIMENT 1: TRADITIONAL MACHINE LEARNING (FULL EPOCHS)
======================================================================
âœ… Data loaded successfully!
   Feature matrix shape: (439, 417)
   Ratings shape: (439,)
   Rating range: [1.0, 5.0]
   Rating mean: 3.04 Â± 1.63
   Number of subjects: 20
   Number of features: 417
   Songs per subject: 21.9

â±ï¸  Starting ML experiments at 00:31:40

============================================================
CLASSIFICATION EXPERIMENT (FULL EPOCHS)
============================================================

--- 2-CLASS CLASSIFICATION ---

ğŸ“Š Original Rating Distribution:
   Rating 1.0: 142 samples (32.3%)
   Rating 3.0: 146 samples (33.3%)
   Rating 5.0: 151 samples (34.4%)

ğŸ“Š New Classification (balanced, 2 classes):
   Low (1-2): 142 samples (32.3%)
   High (3-5): 297 samples (67.7%)

ğŸ” Feature Selection (univariate)...
   Removed 0 constant features
   Remaining features: 417
   Selected 40 features
   Top 5 features by F-score:
   - T4_var: 7.16
   - T4_std: 6.93
   - T4_rms: 6.93
   - P4_mean: 6.87
   - O2_mean: 6.66

ğŸ“Š Train/Test Split:
   Training samples: 351
   Test samples: 88
   Training class distribution: {0: 114, 1: 237}
   Test class distribution: {0: 28, 1: 60}

ğŸ¤– Training Random Forest (2-class)...
   Best parameters: {'max_depth': None, 'n_estimators': 50}
   CV Accuracy: 0.661 Â± 0.011
   Test Accuracy: 0.693
   Test Precision: 0.685
   Test Recall: 0.693
   Test F1-Score: 0.596

ğŸ¤– Training SVM (2-class)...
   Best parameters: {'C': 10, 'gamma': 'scale'}
   CV Accuracy: 0.593 Â± 0.025
   Test Accuracy: 0.602
   Test Precision: 0.606
   Test Recall: 0.602
   Test F1-Score: 0.604

ğŸ¤– Training Logistic Regression (2-class)...
   Best parameters: {'C': 1}
   CV Accuracy: 0.530 Â± 0.046
   Test Accuracy: 0.580
   Test Precision: 0.632
   Test Recall: 0.580
   Test F1-Score: 0.594

ğŸ¤– Training Extra Trees (2-class)...
   Best parameters: {'max_depth': None, 'n_estimators': 100}
   CV Accuracy: 0.644 Â± 0.021
   Test Accuracy: 0.705
   Test Precision: 0.718
   Test Recall: 0.705
   Test F1-Score: 0.618

ğŸ¤– Training Linear Discriminant Analysis (2-class)...
   CV Accuracy: 0.621 Â± 0.016
   Test Accuracy: 0.682
   Test Precision: 0.636
   Test Recall: 0.682
   Test F1-Score: 0.616

ğŸ¤– Training KNN (2-class)...
   Best parameters: {'n_neighbors': 7}
   CV Accuracy: 0.661 Â± 0.016
   Test Accuracy: 0.602
   Test Precision: 0.543
   Test Recall: 0.602
   Test F1-Score: 0.562
   Saved 2-class classification plots

--- 3-CLASS CLASSIFICATION ---

ğŸ“Š Original Rating Distribution:
   Rating 1.0: 142 samples (32.3%)
   Rating 3.0: 146 samples (33.3%)
   Rating 5.0: 151 samples (34.4%)

ğŸ“Š New Classification (balanced, 3 classes):
   Low (1-2): 142 samples (32.3%)
   Medium (3): 146 samples (33.3%)
   High (4-5): 151 samples (34.4%)

ğŸ” Feature Selection (univariate)...
   Removed 0 constant features
   Remaining features: 417
   Selected 40 features
   Top 5 features by F-score:
   - P3_mean: 4.70
   - P4_mean: 4.20
   - F7_mean: 4.18
   - T4_var: 4.16
   - T4_std: 4.08

ğŸ“Š Train/Test Split:
   Training samples: 351
   Test samples: 88
   Training class distribution: {0: 113, 1: 117, 2: 121}
   Test class distribution: {0: 29, 1: 29, 2: 30}

ğŸ¤– Training Random Forest (3-class)...
   Best parameters: {'max_depth': 5, 'n_estimators': 50}
   CV Accuracy: 0.370 Â± 0.022
   Test Accuracy: 0.398
   Test Precision: 0.402
   Test Recall: 0.398
   Test F1-Score: 0.396

ğŸ¤– Training SVM (3-class)...
   Best parameters: {'C': 10, 'gamma': 'scale'}
   CV Accuracy: 0.373 Â± 0.016
   Test Accuracy: 0.318
   Test Precision: 0.321
   Test Recall: 0.318
   Test F1-Score: 0.319

ğŸ¤– Training Logistic Regression (3-class)...
   Best parameters: {'C': 0.1}
   CV Accuracy: 0.336 Â± 0.028
   Test Accuracy: 0.443
   Test Precision: 0.443
   Test Recall: 0.443
   Test F1-Score: 0.443

ğŸ¤– Training Extra Trees (3-class)...
   Best parameters: {'max_depth': 5, 'n_estimators': 50}
   CV Accuracy: 0.353 Â± 0.011
   Test Accuracy: 0.386
   Test Precision: 0.379
   Test Recall: 0.386
   Test F1-Score: 0.377

ğŸ¤– Training Linear Discriminant Analysis (3-class)...
   CV Accuracy: 0.339 Â± 0.028
   Test Accuracy: 0.432
   Test Precision: 0.434
   Test Recall: 0.432
   Test F1-Score: 0.431

ğŸ¤– Training KNN (3-class)...
   Best parameters: {'n_neighbors': 7}
   CV Accuracy: 0.348 Â± 0.041
   Test Accuracy: 0.375
   Test Precision: 0.374
   Test Recall: 0.375
   Test F1-Score: 0.369
   Saved 3-class classification plots

============================================================
REGRESSION EXPERIMENT (FULL EPOCHS)
============================================================

ğŸ” Feature Selection (univariate)...
   Removed 0 constant features
   Remaining features: 417
   Selected 40 features
   Top 5 features by F-score:
   - P3_mean: 4.70
   - P4_mean: 4.20
   - F7_mean: 4.18
   - T4_var: 4.16
   - T4_std: 4.08

ğŸ“Š Train/Test Split:
   Training samples: 351
   Test samples: 88

ğŸ¤– Training Random Forest...
   Best parameters: {'max_depth': 5, 'n_estimators': 50}
   Train MSE: 1.174, RÂ²: 0.563
   Test MSE: 3.249, RÂ²: -0.254

ğŸ¤– Training SVR...
   Best parameters: {'C': 0.1, 'gamma': 'auto'}
   Train MSE: 2.533, RÂ²: 0.057
   Test MSE: 2.680, RÂ²: -0.034

ğŸ¤– Training Ridge...
   Best parameters: {'alpha': 10}
   Train MSE: 2.425, RÂ²: 0.098
   Test MSE: 2.969, RÂ²: -0.146

ğŸ¤– Training Lasso...
   Best parameters: {'alpha': 0.1}
   Train MSE: 2.557, RÂ²: 0.049
   Test MSE: 2.701, RÂ²: -0.043
   Saved regression plots

============================================================
CROSS-SUBJECT VALIDATION (FULL EPOCHS)
============================================================

ğŸ“Š Original Rating Distribution:
   Rating 1.0: 142 samples (32.3%)
   Rating 3.0: 146 samples (33.3%)
   Rating 5.0: 151 samples (34.4%)

ğŸ“Š New Classification (balanced, 3 classes):
   Low (1-2): 142 samples (32.3%)
   Medium (3): 146 samples (33.3%)
   High (4-5): 151 samples (34.4%)

ğŸ” Feature Selection (univariate)...
   Removed 0 constant features
   Remaining features: 417
   Selected 30 features
   Top 5 features by F-score:
   - P3_mean: 4.70
   - P4_mean: 4.20
   - F7_mean: 4.18
   - T4_var: 4.16
   - T4_std: 4.08

ğŸ”„ Leave-one-subject-out validation for 20 subjects...
   s01: 0.400 (20 test samples)
   s02: 0.185 (27 test samples)
   s03: 0.407 (27 test samples)
   s04: 0.300 (10 test samples)
   s05: 0.370 (27 test samples)
   s06: 0.533 (15 test samples)
   s07: 0.308 (26 test samples)
   s08: 0.375 (24 test samples)
   s09: 0.280 (25 test samples)
   s10: 0.310 (29 test samples)
   s11: 0.391 (23 test samples)
   s12: 0.400 (30 test samples)
   s13: 0.500 (18 test samples)
   s14: 0.440 (25 test samples)
   s15: 0.353 (17 test samples)
   s16: 0.407 (27 test samples)
   s17: 0.300 (10 test samples)
   s18: 0.320 (25 test samples)
   s19: 0.364 (22 test samples)
   s20: 0.167 (12 test samples)

ğŸ“Š Cross-subject performance:
   Mean accuracy: 0.356
   Std accuracy: 0.087
   Min accuracy: 0.167
   Max accuracy: 0.533
   Valid subjects: 20/20
   Saved cross-subject validation plot

======================================================================
FINAL FULL EPOCH EXPERIMENT SUMMARY
======================================================================

ğŸ¯ 2_CLASS CLASSIFICATION RESULTS:
--------------------------------------------------
Random Forest             | Accuracy: 0.693 | F1: 0.596
SVM                       | Accuracy: 0.602 | F1: 0.604
Logistic Regression       | Accuracy: 0.580 | F1: 0.594
Extra Trees               | Accuracy: 0.705 | F1: 0.618
Linear Discriminant Analysis | Accuracy: 0.682 | F1: 0.616
KNN                       | Accuracy: 0.602 | F1: 0.562

ğŸ† Best 2_class Classifier: Extra Trees (Accuracy: 0.705)

ğŸ¯ 3_CLASS CLASSIFICATION RESULTS:
--------------------------------------------------
Random Forest             | Accuracy: 0.398 | F1: 0.396
SVM                       | Accuracy: 0.318 | F1: 0.319
Logistic Regression       | Accuracy: 0.443 | F1: 0.443
Extra Trees               | Accuracy: 0.386 | F1: 0.377
Linear Discriminant Analysis | Accuracy: 0.432 | F1: 0.431
KNN                       | Accuracy: 0.375 | F1: 0.369

ğŸ† Best 3_class Classifier: Logistic Regression (Accuracy: 0.443)

ğŸ“ˆ REGRESSION RESULTS:
----------------------------------------
Random Forest             | RÂ²: -0.254 | MSE: 3.249
SVR                       | RÂ²: -0.034 | MSE: 2.680
Ridge                     | RÂ²: -0.146 | MSE: 2.969
Lasso                     | RÂ²: -0.043 | MSE: 2.701

ğŸ† Best Regressor: SVR (RÂ²: -0.034)

ğŸ”„ CROSS-SUBJECT VALIDATION:
----------------------------------------
Mean Accuracy: 0.356 Â± 0.087
Range: [0.167, 0.533]
Valid Subjects: 20

======================================================================
COMPARISON: FULL EPOCHS vs 10-SECOND EPOCHS
======================================================================

ğŸ“Š EXPECTED DIFFERENCES:
----------------------------------------
Full Epochs:
  âœ… Captures complete song response
  âœ… Better for sustained preference patterns
  âœ… Less noisy (longer integration time)
  âŒ Fewer training samples
  âŒ Less suitable for temporal dynamics

10-Second Epochs:
  âœ… More training samples (8x more)
  âœ… Better for machine learning
  âœ… Captures temporal dynamics
  âŒ May miss sustained patterns
  âŒ Potentially more noisy

ğŸ’¡ RECOMMENDATIONS:
----------------------------------------
â€¢ Use FULL EPOCHS for:
  - Understanding sustained neural responses
  - Analyzing complete song preferences
  - Studying long-term EEG patterns
â€¢ Use 10S EPOCHS for:
  - Training robust ML models
  - Real-time applications
  - Temporal pattern analysis

âœ… All full epoch experiments complete! (Total time: 35.2s)
âœ… Results saved to '/home/anower/All/Python/Thesis_VS_code/full_epoch/Figure/ml_experiment2_full.pkl'
âœ… Visualizations saved to '/home/anower/All/Python/Thesis_VS_code/full_epoch/Figure/'

======================================================================
FINAL INSIGHTS: FULL EPOCH ANALYSIS
======================================================================

ğŸ¯ BEST PERFORMANCE:
   2_class: 70.5% (Extra Trees)
   3_class: 44.3% (Logistic Regression)
   Regression: RÂ² = -0.034 (SVR)
   Cross-Subject: 35.6% Â± 8.7%

ğŸ“ˆ RESEARCH CONTRIBUTION:
   âœ… Demonstrated EEG-based music preference prediction from full songs
   âœ… Comprehensive feature extraction from sustained neural responses
   âœ… Robust validation across multiple subjects
   âœ… Comparison framework for different epoch lengths

ğŸ’¡ FOR THESIS:
   â€¢ Full epochs provide insights into sustained neural preferences
   â€¢ Different from 10s epochs - captures complete song processing
   â€¢ Valuable for understanding music cognition mechanisms
   â€¢ Complements shorter epoch analysis for comprehensive study

âš–ï¸ CLASS BALANCE INSIGHTS:
   â€¢ Original ratings show natural preference distribution
   â€¢ Binary classification (Low vs High) may be more robust
   â€¢ Consider using class weights to handle imbalance
   â€¢ Alternative: Use regression instead of classification
