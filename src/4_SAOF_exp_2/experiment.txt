The machine learning pipeline evaluates six classification models—Random Forest, Support Vector Machine (SVM), Logistic Regression, Extra Trees, Linear Discriminant Analysis (LDA), and K-Nearest Neighbors (KNN)—and four regression models—Random Forest, SVR, Ridge, and Lasso. Feature selection retains the top 40 features for standard validation and 30 for LOSO, based on ANOVA F-scores, with key features including F3_temporal_trend,Fp2_p75, P4_mean, and F3_beta_peak_freq. Classification models use balanced class weighting to address minor imbalances, with performance metrics including accuracy, precision, recall, and weighted F1-score. Regression models are tuned using negative mean squared error, assessed via Mean Squared Error (MSE) and R². Hyperparameter tuning employs grid search with 5-fold cross-validation, with optimal parameters for the 0.5x dataset reported as: Random Forest (2-class: n_estimators=100, max_depth=10; 3-class: n_estimators=50, max_depth=10), SVM (2-class: C=10, gamma=‘auto’; 3-class: C=10, gamma=‘scale’), and others as detailed in the methodology.
A stratified 80/20 train-test split produces 351 training and 88 test samples for the original dataset, and 525 training and 132 test samples for the 0.5x augmented dataset. LOSO cross-validation tests generalization across 20 subjects, with per-subject performance visualized in Figure loso_heatmap_2class.png for 2-class and extended to 3-class and regression tasks. The computational cost is manageable (4.3s for synthetic data generation, 100.5s for ML experiments), supporting practical application in EEG research.
